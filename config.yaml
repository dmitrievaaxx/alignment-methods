# config.yaml
name: successful-training
desc: Simple working version

cmd: |
  export PIP_ROOT_USER_ACTION=ignore
  export USE_FLASH_ATTENTION=0
  export TRANSFORMERS_USE_FLASH_ATTENTION=0
  pip install --upgrade pip
  pip install torch==2.5.1+cu121 torchvision==0.20.1+cu121 torchaudio==2.5.1+cu121 --index-url https://download.pytorch.org/whl/cu121
  pip install packaging ninja wheel setuptools psutil
  pip install flash-attn>=2.5.0 --no-build-isolation --no-cache-dir
  pip install accelerate transformers datasets peft wandb hydra-core omegaconf tensordict ray codetiming
  mkdir -p /job
  cp -r ${VERL_DIR} /job/verl
  mkdir -p /job/verl_config
  cp /job/verl/verl/trainer/sft_qwen_1.5b.yaml /job/verl_config/
  python ${UPDATE_CONFIG} /job/verl_config sft_qwen_1.5b
  cd /job/verl && PYTHONPATH=/job/verl:$PYTHONPATH python ${DISABLE_FLASH_ATTN} && PYTHONPATH=/job/verl:$PYTHONPATH torchrun --standalone --nnodes=1 --nproc_per_node=1 verl/trainer/fsdp_sft_trainer.py --config-path /job/verl_config --config-name sft_qwen_1.5b data.train_files="[${TRAIN_DATA}]" data.val_files="[${VAL_DATA}]"

inputs:
  - train_data.parquet: TRAIN_DATA
  - val_data.parquet: VAL_DATA
  - update_config.py: UPDATE_CONFIG
  - verl: VERL_DIR
  - disable_flash_attn.py: DISABLE_FLASH_ATTN

outputs:
  - output_dir: OUTPUT_DIR

cloud-instance-type: g1.1
